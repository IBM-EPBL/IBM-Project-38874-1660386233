# IMPORTS
# -------

# Standard libraries
from importlib import reload
import pathlib
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import itertools
import ipdb
import string
import re-

# 3rd party imports
import numpy as np
import pandas as pd
warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)

from scipy import stats
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import fcluster

import scikit_posthocs as sp

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.dummy import DummyClassifier
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.feature_selection import SelectFromModel

import scikit_posthocs as sp

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.dummy import DummyClassifier
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.feature_selection import SelectFromModel

# Supply your service account key
service_account = pathlib.Path(
    ('path/to/your/service/account/key')
)

# Authenticate and initiate biqquery client
client = bigquery.Client.from_service_account_json(service_account.absolute())

# Get dataset reference
project = 'bigquery-public-data'
dataset_id = 'google_analytics_sample'
dataset_ref = client.dataset(dataset_id, project=project)
dataset = BigqueryDataset(client, dataset_ref)

# Investigate dataset schema
display(dataset.schema.shape)
display(dataset.schema['table_name'].head(n=3))
display(dataset.schema['table_name'].tail(n=3))
# Investigate table schema
# Load BigQuery export schema
bq_exp_schema = pd.read_excel('google_analytics_schema.xlsx', 
                              sheet_name='bq_exp_schema')

# Load google analytics sample table
table_id = 'ga_sessions_20170801'
table_ref = dataset_ref.table(table_id)
table = client.get_table(table_ref)

# Recast table to custom BigqueryTable class
table.__class__ = BigqueryTable

# Load google analytics schema and merge it with export schema
# to get field descriptions
ga_schema = table.schema_to_dataframe(bq_exp_schema)
table.display_schema(ga_schema)

# Select variables and analyze scales
# ----------------------------------

# Discard variables with 1 level
schema_multi_levels = (
    ga_schema[(ga_schema['Num of Levels'] > 1) 
              | (ga_schema['Field Name'] == 'totals.visits')]
    .set_index('Field Name')
)
table.display_schema(schema_multi_levels.drop(columns=['Levels']))

# Discard not related or too detailed and too general variables 
# (done manually in excell file)
schema = (pd.read_excel('google_analytics_schema.xlsx', 
                         sheet_name='ga_schema_uni_level_excl')
          .set_index('Variable Name'))
display(schema)

# Variable selection, Scale and personas characteristics summary
for column in ['Status', 'Scale', 'Field Group']:
    display(schema.groupby(column)[column].count())

# Only selected variables summary
for column in ['Status', 'Scale', 'Field Group']:
    display(schema[schema['Status'] == 'SELECTED']
            .groupby(column)[column]
            .count())

%%time
# Query selected variables from google analytics dataset
query = '''
SELECT
    date,
    hits.transaction.transactionId AS transaction_id,
    fullVisitorId AS client_id,
    (hits.transaction.transactionRevenue / 1e6) AS revenue
FROM
    `bigquery-public-data.google_analytics_sample.ga_sessions_*`,
    UNNEST(hits) AS hits
WHERE
    _TABLE_SUFFIX BETWEEN @start_date AND @end_date
    AND hits.transaction.transactionRevenue IS NOT NULL
ORDER BY
    date
'''

query_params = [
    bigquery.ScalarQueryParameter(
        'start_date', 'STRING', 
        dataset.schema['table_name'].values[0].split('_')[-1]),
    bigquery.ScalarQueryParameter(
        'end_date', 'STRING', dataset.schema['table_name'].values[-2].split('_')[-1])
]

job_config = bigquery.QueryJobConfig()
job_config.query_parameters = query_params
df = client.query(query, job_config=job_config).to_dataframe()

# Cast variables
df['date'] = pd.to_datetime(df['date'])
df['client_id'] = df['client_id'].astype(str)

# Prints dataframe charactersitics
display(df.head())
display(df.info())
display(df.describe(include=np.number))
display(df.describe(exclude=np.number))

# Calcuate RFM variables
present = df['date'].max()
rfm = df.groupby('client_id').agg({
    'date': {'first_purchase':lambda date: (present - date.min()).days,
             'recency': lambda date: (present - date.max()).days},
    'transaction_id': {'frequency':  lambda id_: len(id_)},
    'revenue': {'monetary': lambda revenue: revenue.sum()}})
rfm.columns = rfm.columns.droplevel()

display(rfm.head())
display(rfm.describe())

# Plot distributions
sns.pairplot(rfm);

# Apply log transformation to exponential and chi-squared distributions
rfm[['ln_frequency', 'ln_monetary']] = np.log1p(rfm[['frequency', 'monetary']])
sns.pairplot(rfm[['frequency', 'ln_frequency', 'monetary', 'ln_monetary']],
             diag_kws=dict(log=True));

# Investigate multicolinearity
sns.heatmap(rfm.corr(), annot=True);

# Investigate oultiers based on multivariate dissimilarity
# --------------------------------------------------------

# Create original and transformed variable sets
idxs = {'original': pd.Index(['first_purchase', 'recency', 
                              'frequency', 'monetary']),
        'transformed': pd.Index(['first_purchase', 'recency', 
                                 'ln_frequency', 'ln_monetary'])}

# Calculate dissimilarity for both sets
dissimilarity = pd.DataFrame()
for idx_name, idx in idxs.items():
    dissimilarity[idx_name] = helper.get_dissimilarity(rfm[idx])

# Print distributions of dissimilarities
axes = dissimilarity.hist(log=True)
plt.suptitle('Distribution of Customers Dissimilarity')
for ax in axes[0]:
    ax.set_xlabel('Dissimilarity')
axes[0, 0].set_ylabel('Log # of customers')
plt.show()

# Display overview
ext_summary = (pd.concat([rfm[idxs['original']], dissimilarity], 
                             axis=1)
                 .sort_values('transformed', ascending=False))
display(ext_summary[:10])
display(dissimilarity.describe())

# Monetary value of outliers
display((ext_summary['monetary'][1].sum() / rfm['monetary'].sum()).round(3))

# Randomly split data into two sets, and keep outliers in each set
# -----------------------------------------------------------------

# Make results deterministic
random_state=1

# Select variables
X = rfm.drop(columns=['first_purchase'])
for idx_name, idx in idxs.items():
    idxs[idx_name] = idx.drop('first_purchase', errors='ignore')

# Split datasets
threshold = 10
X_names = ['train', 'test']
size = 0.5
Xs = helper.split_data(X, idxs['transformed'], X_names, threshold, 
                       test_size=size, random_state=random_state)

for X_name, X in Xs.items():
    print('Dataset', X_name, 'of shape', X.shape)

# BUILD AND OPTIMIZE MODEL FOR 1st DATASET
# ----------------------------------------

# Standardize datasets
scalers={}
Xs_std = {}
idxs['all_inputs'] = pd.Index([
    'recency', 'frequency', 'monetary', 'ln_frequency', 'ln_monetary', 
    'recency_std', 'frequency_std', 'monetary_std','ln_frequency_std', 
    'ln_monetary_std'
])


for X_name, X in Xs.items():
    scalers[X_name] = StandardScaler().fit(X)
    Xs_std[X_name] = scalers['train'].transform(X)
    Xs_std[X_name] = pd.DataFrame(Xs_std[X_name], 
                                  columns=Xs[X_name].columns+'_std', 
                                  index=Xs[X_name].index)
    Xs[X_name] = (pd.concat([Xs[X_name], Xs_std[X_name]], axis=1)
                  .reindex(idxs['all_inputs'], axis=1))

# Agglomerative Clustering with average linkage
idxs['transformed_std'] = pd.Index(
    ['recency_std', 'ln_frequency_std', 'ln_monetary_std'])

agl_cluster = linkage(Xs_std['train'][idxs['transformed_std']], 'ward')


# Construct dendrogram
plt.figure(figsize=(6, 6))
dn = dendrogram(agl_cluster, p=30, truncate_mode='lastp', orientation='right')
plt.title('Customers Dendrogram')
plt.xlabel('Aglomeration Coeficient')
plt.ylabel('Customers')
plt.show()

# Step 1: Inspect outliers in agglomeration schedule if any
pass

# Step 2: Consider removing outliers if appropriate
pass

# Step 3: Run algomerative clustering again go to step 1
pass

# Note: no outliers
# Determine number of  candidate solutions
# ---------------------------------------

# Get aglomeration shedule
agl_schedule = pd.DataFrame(
    agl_cluster,
    columns=['cluster1', 'cluster2', 'coefficient', 'cluster_size'],
    index=pd.Index(range(1, Xs_std['train'].shape[0]), name='stage')
)

# Calculate proportional heterogeneity increase
agl_schedule['num_of_clusters'] = list(reversed(agl_schedule.index))
agl_schedule['prop_heterogeneity_increase'] = (
    agl_schedule['coefficient']
    .diff()
    .shift(periods=-1)
    / agl_schedule['coefficient']
)

# Print last x stages of algomeration schedule
stages = 15
display(agl_schedule[-stages:])

# Print heterogenity scree plot
(agl_schedule
 .set_index('num_of_clusters')
 ['prop_heterogeneity_increase'][-stages:]
 .plot(title='Heterogeneity Scree Plot'))
plt.ylabel('Proporion heterogeneity increase to next stage');


# Selected candidate solutions
num_seeds = [3, 6]

# Profile candiate solutions
# --------------------------

for k in num_seeds:
    
    # Generate candidate solution labels
    labels = fcluster(agl_cluster, k, criterion='maxclust')
    Xs['train']['hierarchic_clusters_{}'.format(k)] = labels
    
    # Display cluster sizes
    display(Xs['train'].groupby('hierarchic_clusters_{}'.format(k))
                       .size()
                       .rename('cluster_sizes'))

display(Xs['train'].head())

# Test candidate solutions
# ---------------------------

# Initiate profile function arguments
idxs['hierarchic_clusters'] = Xs['train'].columns[
    Xs['train'].columns.str.contains('hierarchic_clusters')]

post_hoc_test = sp.posthoc_nemenyi

# Execute cluster profile tests, print cluster profiles & scatterplots
for solution in idxs['hierarchic_clusters']:
    summary, post_hoc, prof_ax, clst_pg = (
        helper.analyze_cluster_solution(
            Xs['train'], idxs['transformed_std'], 
            solution, post_hoc_fnc=post_hoc_test
        )
    )
    

 # Print original variable means and averages per cluster
    str_ = 'Profile characteristics'
    print(str_ + '\n' + '-' * len(str_))

    display(Xs['train']
            .groupby(solution)
            [idxs['original']]
            .agg(['mean', 'median'])
            .round(1)
            .swaplevel(axis=1)
            .sort_index(level=0, axis=1))
    
    print('\n')

# Revenue overview of 6 cluster solution
monetary_cls_6 = (Xs['train']
                  .groupby('hierarchic_clusters_6', as_index=False)
                  ['monetary']
                  .sum())
monetary_cls_6['prop_monetary'] = (monetary_cls_6['monetary'] 
                                   / Xs['train']['monetary'].sum())
display(monetary_cls_6)

# K-mean clustering for selected candidate solutions
kmeans, labels = {}, {}
for k in num_seeds:
    
    # Fit model
    kmeans['non_sorted_{}'.format(k)] = (
        KMeans(n_clusters=k, 
               random_state=random_state, 
               init='k-means++')
        .fit(Xs['train'][idxs['transformed_std']])
    )
    
    # Predict lables
    labels['non_sorted_{}'.format(k)] = (
        kmeans['non_sorted_{}'.format(k)]
        .predict(Xs['train'][idxs['transformed_std']])
    )
    
    Xs['train']['nonhierarchic_clusters_{}'.format(k)] = (
        labels['non_sorted_{}'.format(k)]+1
    )
    
# Display cluster sizes
idxs['nonhierarchic_clusters'] = Xs['train'].columns[
    Xs['train'].columns.str.contains('nonhierarchic_clusters')]

for solution in idxs['nonhierarchic_clusters']:
    display(Xs['train'].groupby(solution)
                      .size()
                      .rename('cluster_sizes')) 

# Test candidate solutions
# ---------------------------

# Execute cluster profile tests, print cluster profiles & scatterplots
for solution in idxs['nonhierarchic_clusters']:
    summary, post_hoc, prof_ax, clst_pg = (
        helper.analyze_cluster_solution(
            Xs['train'], idxs['transformed_std'], 
            solution, post_hoc_fnc=post_hoc_test
        )
    )
    
    # Print original variable means and averages per cluster
    str_ = 'Profile characteristics'
    print(str_ + '\n' + '-' * len(str_))

    display(Xs['train']
            .groupby(solution)
            [idxs['original']]
            .agg(['mean', 'median'])
            .round(1)
            .swaplevel(axis=1)
            .sort_index(level=0, axis=1))
    
    print('\n')

# Evaluate within clusters homogeneity and 
# between clusters heterogeneity using Silhouette coeficient
idxs['clusters'] = Xs['train'].columns[
    Xs['train'].columns.str.contains('clusters')]

score = pd.Series(index=idxs['clusters'], name='silhouette_score')

for k, solution in zip(num_seeds * 2, idxs['clusters']):
    score[solution] = silhouette_score(Xs['train'][idxs['transformed_std']],
                                       Xs['train'][solution], 
                                       random_state=random_state)
# Diplay overview
score = score.sort_values(ascending=False)
score.plot.bar(title = 'Silhouette coefficient')
plt.show()
display(score)


# initiate buyer personas names
cluster_names = {3: ['frequent shoppers and high spenders',
                     'customer churn',
                     'new customers'],
                 6: ['new low spenders',
                     'moderate spenders churn',
                     'frequent shoppers and moderate spenders',
                     'one time moderate spenders',
                     'frequent shoppers and huge spenders',
                     'low spenders churn']}

# Generate sorted dataset by monetary value
Xs['train_sorted'] = Xs['train'].sort_values('monetary_std').copy()

# K-mean clustering for selected candidate solutions
for k in num_seeds:
    
    # Fit model
    kmeans['sorted_{}'.format(k)] = (
        KMeans(n_clusters=k, 
               random_state=random_state, 
               init='k-means++')
        .fit(Xs['train_sorted'][idxs['transformed_std']])
    )

 # Predict lables
    labels['sorted_{}'.format(k)] = (
        kmeans['sorted_{}'.format(k)]
        .predict(Xs['train_sorted'][idxs['transformed_std']])
    )
    
    Xs['train_sorted']['nonhierarchic_sorted_clusters_{}'.format(k)] = (
        labels['sorted_{}'.format(k)] + 1
    )

    # Cross tabulation
    cross_tab, missmatch, total_missmatch = helper.get_missmatch(
        index=Xs['train']['hierarchic_clusters_{}'.format(k)],
        columns=Xs['train_sorted']['nonhierarchic_sorted_clusters_{}'.format(k)],
        rownames=['original'],
        colnames=['sorted']
    )

    print(cross_tab,'\n')
    print(missmatch, '\n')
    print('Total missmatch proportion: {:.2f}\n\n\n'.format(total_missmatch))
 
# Validate solution generalizability to whole population
# ------------------------------------------------------
for k in num_seeds:
    
    kmeans['train_{}'.format(k)] = kmeans['non_sorted_{}'.format(k)]
    
    # Fit model on test dataset
    kmeans['test_{}'.format(k)] = (
        KMeans(n_clusters=k, 
               random_state=random_state, 
               init='k-means++')
        .fit(Xs['test'][idxs['transformed_std']])
    )
    
    # Predict lables with model fitted on test dataset
    labels['test_true_{}'.format(k)] = (
        kmeans['test_{}'.format(k)]
        .predict(Xs['test'][idxs['transformed_std']])
    )
    
    Xs['test']['true_clusters_{}'.format(k)] = (
        labels['test_true_{}'.format(k)] + 1
    )

    # Predict lables with model fitted on train dataset
    labels['test_predicted_{}'.format(k)] = (
        kmeans['train_{}'.format(k)]
        .predict(Xs['test'][idxs['transformed_std']])
    )
    
    Xs['test']['predicted_clusters_{}'.format(k)] = (
        labels['test_predicted_{}'.format(k)] + 1
    )
    
    # Cross tabulation
    cross_tab, missmatch, total_missmatch = helper.get_missmatch(
        index=Xs['test']['true_clusters_{}'.format(k)],
        columns=Xs['test']['predicted_clusters_{}'.format(k)],
        rownames=['true'],
        colnames=['predicted']
 )

    print(cross_tab,'\n')
    print(missmatch, '\n')
    print('Total missmatch proportion: {:.2f}\n\n\n'.format(total_missmatch))

# Predict cluster labels for whole dataset
# ----------------------------------------

# Standardize data
X = rfm.drop(columns=['first_purchase'])
X_std = scalers['train'].transform(X)
X_std = pd.DataFrame(X_std, 
                     columns=X.columns+'_std', 
                     index=X.index)
X = (pd.concat([X, X_std], axis=1)
              .reindex(idxs['all_inputs'], axis=1))

# Predict labels
labels = {}
for k in num_seeds:
    labels[k] = (kmeans['train_{}'.format(k)]
                 .predict(X[idxs['transformed_std']]))
    
    X['clusters_{}'.format(k)] = labels[k] + 1

y_pred = X[X.columns[X.columns.str.contains('clusters')]]
display(y_pred)

%%time
# Query additional variables from google analytics dataset
query = '''
SELECT
    CONCAT(fullVisitorId, CAST(visitId AS STRING)) AS session_id,
    visitNumber AS visit_number,
    date,
    totals.visits AS visits,
    totals.pageviews AS pageviews,
    totals.timeOnSite AS time_on_site,
    trafficSource.referralPath AS referral_path,
    trafficSource.campaign AS campaign,
    trafficSource.source AS source,
    trafficSource.medium AS medium,
    trafficSource.keyword AS traffic_keyword,
    trafficSource.adContent AS ad_content,
    trafficSource.adwordsClickInfo.page AS ad_page,
    trafficSource.adwordsClickInfo.slot AS ad_slot,
    trafficSource.adwordsClickInfo.adNetworkType AS ad_network_type,
    trafficSource.isTrueDirect AS direct_traffic,
    device.browser AS browser,
    device.operatingSystem AS operating_system,
    device.deviceCategory AS device_category,
    geoNetwork.continent AS continent,
    geoNetwork.subContinent AS subcontinent,
    geoNetwork.country AS country,
    geoNetwork.region AS region,
    geoNetwork.metro AS metro,
    geoNetwork.city AS city,
    geoNetwork.networkDomain AS domain,
    customDimensions.value AS sales_region,
    hits.hour AS hour,
    hits.transaction.transactionId AS transaction_id,
    hits_product.productSKU AS product_sku,
    hits_product.v2ProductName AS product_name,
    hits_product.v2ProductCategory AS product_category,
    hits_product.productBrand AS product_brand,
    (hits_product.productPrice / 1e6) AS product_price,
    hits_product.productQuantity AS product_quantity,
    hits.social.hasSocialSourceReferral AS is_social_referral,
    hits.social.socialInteractionAction AS social_action,
    hits.social.socialInteractionNetwork AS social_interaction_network,
 hits.social.socialInteractionNetworkAction AS network_action,
    hits.social.socialInteractions as social_interactions,
    hits.social.socialNetwork as social_network,
    hits.social.uniqueSocialInteractions as unique_social_interactions,
    hits.contentGroup.contentGroup1 AS product_brand_grp,
    hits.contentGroup.contentGroup2 AS product_category_grp,
    fullVisitorId AS client_id,
    channelGrouping AS channel_group
FROM
    `bigquery-public-data.google_analytics_sample.ga_sessions_*`
    LEFT JOIN UNNEST(customDimensions) AS customDimensions
    LEFT JOIN UNNEST(hits) AS hits
    LEFT JOIN UNNEST(hits.product) AS hits_product
WHERE
    _TABLE_SUFFIX BETWEEN @start_date AND @end_date
    AND hits.transaction.transactionRevenue IS NOT NULL
    AND hits_product.productSku IS NOT NULL
ORDER BY
    date
'''

job_config = bigquery.QueryJobConfig()
job_config.query_parameters = query_params
df = client.query(query, job_config=job_config).to_dataframe()

# get scale indexes
identifiers = schema[(schema['Status'] == 'SELECTED')
                     & (schema['Scale'] == 'IDENTIFIER')].index
identifiers = identifiers.union(pd.Index(['session_id']))

nominal = schema[(schema['Status'] == 'SELECTED')
                 & (schema['Scale'] == 'NOMINAL')].index

binary = schema[(schema['Status'] == 'SELECTED')
                & (schema['Scale'] == 'BINARY')].index

numeric = schema[(schema['Status'] == 'SELECTED')
                 & (schema['Scale'] == 'NUMERIC')].index

timestamp = schema[(schema['Status'] == 'SELECTED')
                   & (schema['Scale'] == 'TIMESTAMP')].index

# Cast variables to proper dtypes
df[numeric] = df[numeric].astype('float')
df[identifiers] = df[identifiers].astype(str)
df[nominal] = df[nominal].astype('category')
df[timestamp] = pd.to_datetime(df[timestamp].squeeze())

# Prints dataframe charactersitics
with pd.option_context('display.max_columns', None):
    display(df.head())
    
display(df.info())
display(df.describe(include=np.number))

with pd.option_context('display.max_columns', None):
    display(df.describe(exclude=np.number))
    

# print scales summary 
print('Variables Scale Summary:')
display(schema
        .loc[schema['Status'] == 'SELECTED', 'Scale']
        .value_counts())

# HANDLE MISSING VALUES
# ---------------------

# Plot missing values patterns
nan_chars = {'nan',
             'None',
             '(none)',
             '(not set)',
             'not available in demo dataset'}
             
nan_mask = (df.isna() 
            | df.isnull() 
            | (df == 'nan')
            | (df == 'None')
            | (df == '(none)') 
            | (df == '(not set)') 
            | (df == 'not available in demo dataset'))
plt.figure(figsize=(15, 15))
ax = sns.heatmap(nan_mask, cbar=False);
plt.title('Missing Data Patterns - White is missing')
plt.show()

# Plot missing values proportion across variables and cases
nan_prop_vars = ((nan_mask.sum() / nan_mask.index.size)
                 .rename('proprotion of missing values in variables')
                 .sort_values(ascending=False))

nan_prop_cases = ((nan_mask.sum(axis=1) / nan_mask.columns.size)
                 .rename('proprotion of missing values in cases')
                 .sort_values(ascending=False))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))
nan_prop_vars.hist(ax=ax1)
ax1.set_title('Proportion of missing values in variables')
nan_prop_cases.hist(ax=ax2)
ax2.set_title('Proprotion of missing values in cases');


# Analyze proportion of missing values and missing vlaue codes in variables
def analyze_nan_levels(var, nan_chars):
    levels = var.unique()
    nan_levels = set(levels) & nan_chars | set(levels[pd.isna(levels)])
    return pd.Series({'num_levels': len(levels),
                      'levels': set(levels),
                      'num_nan_levels': len(nan_levels),
                      'nan_levels': nan_levels})

nan_vars_summary = nan_prop_vars.to_frame('nan_proportion')
nan_vars_summary[['num_levels', 
                  'levels', 
                  'num_nan_levels', 
                  'nan_levels']] = (df.apply(analyze_nan_levels, 
                                             args=(nan_chars,))
                                    .T)

display(nan_vars_summary[nan_prop_vars > 0])

# FEATURE ENGINEERING
# -------------------
re_engineered_vars = ['traffic_keyword',
                      'product_brand',
                      'product_category',
                      'sales_region',
                      'avg_visits',
                      'avg_pageviews',
                      'avg_time_on_site',
                      'social_referral',
                      'ad_campaign',
                      'avg_product_revenue',
                      'avg_unique_products']

# Reconstruct traffic variables
# -----------------------------
df['traffic_keyword'] = (df['traffic_keyword']
                         .astype(object)
                         .apply(helper.reconstruct_traffic_keyword)
                         .astype('category'))

# Engineer brand variable from brand keywords in product name
# and brand variables. Remaining (not set) brand is Google brand
# --------------------------------------------------------------
product_df = helper.query_product_info(client, query_params)
df['product_brand'] = (helper.reconstruct_brand(df['product_sku'], product_df)
                       .astype('category'))

# Engineer category labels from category variables and keywords in product name.
# Remaining (not set) brand is predicted by Naive Bayes Model with 
# f1 weighted test score > 0.8% (0.985)
# ------------------------------------------------------------------------------

# load mappings from category variables to category_label
product_category = (
    pd.read_excel('product_categories.xlsx', 
                  sheet_name='product_category_id')
    [['product_category', 'category_label']]
)

product_category_grp = pd.read_excel('product_categories.xlsx', sheet_name='product_category_grp_id')

category_spec = {'product_category': product_category,
                 'product_category_grp': product_category_grp}

# reconstruct category labels from category variables and product names
df['product_category'] = (
    helper.reconstruct_category(df['product_sku'], product_df, category_spec)
    .astype('category')
)

# reconstruct sales region from subcontinent labels
# -------------------------------------------------
df['sales_region'] = (df['subcontinent']
                       .apply(helper.reconstruct_sales_region)
                       .astype('category'))

# cast binary variable is_social_referral
df['social_referral'] = ((df['is_social_referral']
                          .replace(['Yes', 'No'], [True, False]))
                          if df['is_social_referral'].dtype != 'bool'  
                          else df['is_social_referral'])

# create ad campaign flag from campaign and add variables
ad_vars = ['campaign', 'ad_content', 'ad_page', 'ad_slot', 'ad_network_type']
df['ad_campaign'] = False
df.loc[~nan_mask[ad_vars].all(axis=1), 'ad_campaign'] = True

# drop cases and variables with high proportion of missing values
# ---------------------------------------------------------------
# Variables with high proportion of missing values
drop_vars = [
    'visits'                         # constant, does not ad any information
    'referral_path',                 # not valuable information
    'product_name',                  # too detailed, category sufficient
    'product_brand',                 # reconstructed
    'product_brand_grp',             # reconstructed
    'product_category_grp',          # reconstructed
    'is_social_referral',            # reconstructed social_referral
    'traffic_kbeyword',               # to detailed and collinear with channel group
    'campaign',                      # reconstructed is_ad_campaign
    'ad_content',                    # reconstructed is_ad_campaign
    'ad_page',                       # reconstructed is_ad_campaign
    'ad_slot',                       # reconstructed is_ad_campaign
    'ad_network_type',               # reconstructed is_ad_campaign
    'domain',                        # high proportion of missing values
    'city',                          # high proportion of missing values
    'region',                        # high proportion of missing values
    'metro',                         # high proportion of missing values
    'direct_traffic',                # collinear with medium & channel group
    'medium',                        # collinear with channel group
    'social_action',                 # no valid values
    'social_interaction_network',    # no valide values
    'network_action',                # not valuable information
    'social_interactions',           # no valide values
    'unique_social_interactions'     # no valide values
    

]

# cases
cases = (nan_mask[['continent', 
                   'subcontinent', 
                   'country',
                   'time_on_site']]
         .any(axis=1))

# delete variables and cases
clean_df = (df.drop(columns=drop_vars, errors='ignore')
              .drop(index=df.index[cases], errors='ignore'))

# check for missing values
clean_mask = (clean_df.isna() 
                | clean_df.isnull() 
                | (clean_df == 'nan')
                | (clean_df == 'None')
                | (clean_df == '(none)') 
                | (clean_df == '(not set)') 
                | (clean_df == 'not available in demo dataset'))

# encode and aggregate engineered variables on customer level
# -----------------------------------------------------------
agg_df = helper.aggregate_data(clean_df)

# dipslay summary
txt = 'MISSING VALUE AND RE-EGINEERING SUMMARY'
print(txt + '\n' + '-'*len(txt) + '\n' )
txt = ('Missing values: {} ({:.1f}%)'
       .format(nan_mask.sum().sum(),
               nan_mask.sum().sum() / np.product(nan_mask.shape) * 100))       
print(txt + '\n')

txt = ('Re-engineered variables: {} ({:.1f}%)'
       .format(len(re_engineered_vars), 
               len(re_engineered_vars) / df.columns.size * 100))   
print(txt)
print(re_engineered_vars, '\n')

txt = ('Deleteted variables: {} ({:.1f}%)'
       .format(len(drop_vars), 
               len(drop_vars) / df.columns.size * 100))       
print(txt)
print(drop_vars, '\n')

txt = ('Deleteted cases: {} ({:.1f}%)'
       .format(cases.sum(), cases.sum() / df.index.size * 100))       
print(txt)
display(df[cases])

txt = ('\nMissing values: {} ({:.1f}%)'
       .format(clean_df.isna().sum().sum(),
               clean_df.isna().sum().sum() / np.prod(clean_df.shape)))       
print(txt + '\n')

txt = ('ENCODED AND AGGREGATED DATA ON CLIENT LEVEL:')
print(txt, '\n' + '-' * len(txt))
display(agg_df)


# BUILD MACHINE LEARNING MODEL TO PREDICT BUYER PERSONA
# ------------------------------------------------------

# Add buyer personas (customer clusters) to additional data
data = pd.merge(agg_df, 
                y_pred,
                how='left',
                left_index=True,
                right_index=True)

# Split data
X = data[data.columns[~data.columns.str.contains('clusters')]]
Y = data[data.columns[data.columns.str.contains('clusters')]]
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.3, random_state=random_state
)


# Screen Classifiers
# ------------------

# Cycle over cluster solutions
pipe, grid_search = {}, {}
for k in num_seeds:
    # set pipeline
    pipe_steps = [
        ('preprocessor', StandardScaler()),
        ('feature_selector', 
         SelectFromModel(
             RandomForestClassifier(
                 n_jobs=-1, 
                 random_state=random_state
            ),
            threshold = -np.inf
         )
        ),
        ('classifier', None)
    ]
    
    pipe[k] = Pipeline(pipe_steps)

    # set grid_search
    param_grid = {
        'feature_selector__max_features': [8, 20],
        'classifier': [
            RandomForestClassifier(n_jobs=-1, random_state=random_state),
            ExtraTreesClassifier(n_jobs=-1, random_state=random_state),
            AdaBoostClassifier(random_state=random_state),
            GradientBoostingClassifier(random_state=random_state),
            LinearSVC(dual=False, random_state=random_state),
            SVC(random_state=random_state),
            DummyClassifier(random_state=random_state)
        ]
    }
    
    grid_search[k] = GridSearchCV(
        estimator=pipe[k], 
        param_grid=param_grid, 
        scoring='f1_weighted',
        n_jobs=-1,
        return_train_score=True
    )

    # run screening
    title = 'MODEL SCREENING EVALUATION: {} CLUSTERS'.format(k)
    grid_search[k] = helper.screen_model(
        X_train, 
        X_test, 
        Y_train['clusters_{}'.format(k)], 
        Y_test['clusters_{}'.format(k)],
        grid_search=grid_search[k],
        title=title,
        verbose='text'
    )


# fine tune selected classifiers
# ------------------------------
# Cycle over cluster solutions
pipe, grid_search = {}, {}
max_features = [
    np.arange(5, 15, 1),
    np.arange(5, 19, 1)
]
fig, axes = plt.subplots(
    1, len(num_seeds), figsize=(10, 5), sharey=True
)
for i, (ax, k, num_features) in enumerate(zip(
    axes.flatten(), num_seeds, max_features)):
    
    # set pipeline
    pipe_steps = [
        ('preprocessor', StandardScaler()),
        ('feature_selector',
         SelectFromModel(
             RandomForestClassifier(
                 n_jobs=-1, 
                 random_state=random_state
            ),
            threshold = -np.inf
         )
        ),
        ('classifier', None)
    ]
    
    pipe[k] = Pipeline(pipe_steps)

    # set grid_search
    param_grid = {
        'feature_selector__max_features': num_features,
        'classifier': [
            LinearSVC(random_state=random_state, dual=False)
        ]
    }
    
    grid_search[k] = GridSearchCV(
        estimator=pipe[k], 
        param_grid=param_grid, 
 scoring='f1_weighted',
        n_jobs=-1,
        return_train_score=True
    )

    # run screening
    title = 'SOLUTION: {} CLUSTERS'.format(k)
    plt.sca(ax)
    grid_search[k] = helper.screen_model(
        X_train, 
        X_test, 
        Y_train['clusters_{}'.format(k)], 
        Y_test['clusters_{}'.format(k)],
        grid_search=grid_search[k],
        fine_param='param_feature_selector__max_features',
        title=title,
        verbose='plot'
    )

# PROFILING BUYER PERSONA
# -----------------------
X_std = pd.DataFrame(StandardScaler().fit_transform(X),
                     index=X.index, columns=X.columns)
for k in num_seeds:
    feature_names = X.columns[grid_search[k]
                              .best_estimator_
                              .named_steps
                              .feature_selector
                              .get_support()]
    
    title = 'Clusters {}:'.format(k)
    helper.plot_features_significance(
        grid_search[k].best_estimator_['classifier'], 
        X_std,
        Y['clusters_{}'.format(k)],
        feature_names, 
        cluster_names[k], 
        title=title,
        threshold=-np.inf)


